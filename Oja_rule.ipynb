{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oja_rule.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USb5uDYWQ0GJ"
      },
      "source": [
        "# Exercise - Oja's rule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kONCaWtmQunS"
      },
      "source": [
        "# Install matplotlib for plotting results. Numpy is included in the package\n",
        "# sklearn will be used for implementing the standard PCA technique\n",
        "!pip install matplotlib\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzlqgcDhROqP"
      },
      "source": [
        "# Principal Components Analysis (PCA): Simple application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zgTe3kmRGSE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn.decomposition"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jEoc5IoRXdi"
      },
      "source": [
        "Principal components analysis (PCA) is a well-known technique from multivariate statistics used for simplifying systems formed by many variables. After applying PCA to a system with N variables, it is possible to reduce the system to M principal components (PCs) (where M < N), while maintaining most of the relevant information.\n",
        "\n",
        "PCA finds the correlations among the different variables and defines new variables - the PCs - which are obtained by linear combinations of the former.\n",
        "\n",
        "In this exercise, we will create synthetic data for a dummy example with two variables which are highly correlated. Namely, this data corresponds to the model of a simple mechanical spring that follows Hooke's law for elastic materials, i.e., $\\Delta l = F * k$. Let's supose that we have two sensors monitoring a spring and providing the elongation and force applied to the spring. Since these two variables are highly correlated, PCA should be able to detect this correlation and reduce the system to a single principal component (PC).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3PsVDebRViW"
      },
      "source": [
        "k_elastic = 3   # N/m\n",
        "# Generate the real displacement of the spring during the simulation,\n",
        "# and the equivalent noisy measurements from the sensor\n",
        "real_displacement = np.arange(-5, 5, 0.04)\n",
        "measured_displacement = real_displacement + np.random.normal(0, 0.5, 250)\n",
        "\n",
        "# Calculate the ideal force and add noise to the measured variable\n",
        "real_force = k_elastic * real_displacement\n",
        "measured_force = real_force + np.random.normal(0, 1, real_force.size)\n",
        "\n",
        "# Normalize the measured values between -1 and 1\n",
        "measured_displacement /= np.abs(measured_displacement).max()\n",
        "measured_force /= np.abs(measured_force).max()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXc3tGPhj7Ho"
      },
      "source": [
        "Now, we will apply the standard PCA method on this artificially generated data. The PCA should show one principal component carrying most of the information, since the displacement and the force are linearly dependent variables, and the variation is only induced by noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzS_3ymbkPSw"
      },
      "source": [
        "# Apply PCA using the traditional method from sklearn\n",
        "samples = np.vstack((measured_displacement, measured_force)).transpose()\n",
        "pca = sklearn.decomposition.PCA()\n",
        "pca = pca.fit(samples)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdxioAWzkeHw"
      },
      "source": [
        "The variable ```pca``` is an object containing, among others, the eigenvectors and eigenvalues corresponding to the different PCs. Let's visualize now the original data, and the direction of the two principal components that the previous method found"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRZqBWvNk7rW"
      },
      "source": [
        "# Generate a square figure, so the orthonormality can be directly observed \n",
        "fig,ax = plt.subplots(1, figsize=(10,10))\n",
        "eigenvectors = pca.components_\n",
        "eigenvalues = pca.explained_variance_\n",
        "v_1 = eigenvectors[0]\n",
        "v_2 = eigenvectors[1]\n",
        "d_0 = measured_displacement.mean()\n",
        "f_0 = measured_force.mean()\n",
        "plt.scatter(measured_displacement, measured_force)\n",
        "# Print in red the first PC, and in green the second PC\n",
        "plt.arrow(d_0, f_0, v_1[0], v_1[1], color=\"red\", width=0.01)\n",
        "plt.arrow(d_0, f_0, v_2[0], v_2[1], color=\"green\", width=0.01)\n",
        "plt.ylim(-2, 2)\n",
        "plt.xlim(-2, 2)\n",
        "\n",
        "ax.set_yticklabels([])\n",
        "ax.set_xticklabels([])\n",
        "ax.set_xlabel(\"Displacement\", fontsize=20)\n",
        "ax.set_ylabel(\"Force\", fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0T5v6IIq8wX"
      },
      "source": [
        "Let's also see the explained variance by each PC. It is calculated as the relative importance of each corresponding eigenvalue relative to the total sum of all eigenvalues. It can be easily seen that most of the information in the input data is contained in the first PC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJbYNpaqrI5-"
      },
      "source": [
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "  print(\"PC {}: {}\".format(i+1, ratio))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlTmblfIxbyp"
      },
      "source": [
        "In this case the usage of PCA is redundant because we have full knowledge of the system and we could take just one of the two variables manually. However, we often have to deal with problems that involve many variables, which are probably correlated with relationships that we are not aware of.\n",
        "\n",
        "Some more complicated examples where PCA is a very powerful tool are the following:\n",
        "* Summarizing the relevant features that define a human face from an image, and develop a classifier that labels the name of the corresponding person (always respecting the data privacy of the face owners). The PCs of a face image are typically known as _eigenfaces_.\n",
        "* Analyzing the variables from thousands of companies in the stock market, and provide a quantitative measurement of how risky is to invest in a specific sector\n",
        "* Reduce all physiological measures of certain animals into a single score that reflects certain features, e.g., provide an educated guess of the age of a cow by providing the weight, height, hoofs size, hornes length, etc.\n",
        "* Compression of very large datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge47mmzfxXex"
      },
      "source": [
        "# Principal components analysis with Oja's rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8F9KdhHtE23"
      },
      "source": [
        "Let's now apply Oja's rule to the previous spring example. The task is to obtain the eigenvector that corresponds to the first PC of the data, since the system can be described with just the main PC.\n",
        "\n",
        "Let's create a class that allows us to simulate Oja's rule over all the input samples. After the class iterates over all samples, this small neural network should have learned the components of the first PC, and store it in the weights of the network.\n",
        "\n",
        "Oja's rule is a form of unsupervised, Hebbian learning that typically works with data encoded in the form of rates. Therefore, the inputs to our class represent the rates of the network connections. This rule is elegant and simple, as it provides competition and saturation to the calculated weights. On the negavite side, it is hard to think of biological neurons implementing this rule, since that would require them to compute the covariance matrix of all inputs, even before they go through the synaptic weights.\n",
        "\n",
        "In our case, our network is composed by two inputs (force and displacement), and one output (the main principal component)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99FYSZXNtvg-"
      },
      "source": [
        "class OjaNetwork():\n",
        "    \"\"\"\n",
        "    Class containing a model of the Oja's learning rule\n",
        "\n",
        "    The class iterates over the provided samples and learns the weights,\n",
        "    which ideally converge to the eigenvector representing the main\n",
        "    principal component of the distribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, dynamic_learning=False, dims=2):\n",
        "        # With dynamic learning, the learning rate will slowly decrease over time\n",
        "        self.dynamic_learning = dynamic_learning\n",
        "        # Initial learning rate (stays the same if dynamic learning is False)\n",
        "        self.alpha = 0.5\n",
        "        self.t = 0\n",
        "        # Time step between iterations\n",
        "        self.delta_t = 1\n",
        "        # Initialize the weights with random values around 0.1\n",
        "        self.weights = (np.random.rand(dims)/10).reshape((2,1))\n",
        "        self.weights_trace = np.copy(self.weights)\n",
        "\n",
        "    def update_learning_rate(self):\n",
        "        \"\"\"\n",
        "        Update the learning rate of the weight update\n",
        "        \"\"\"\n",
        "        self.alpha = 0.25 / np.sqrt(self.t)\n",
        "        return self.alpha\n",
        "\n",
        "    def update_weights(self, X):\n",
        "        \"\"\"\n",
        "        Update the weights according to Oja's learning rule\n",
        "        \"\"\"\n",
        "        # Oja's rule simple form: delta_w = alpha * y * (X - y*W)\n",
        "        y = np.dot(X.transpose(), self.weights)\n",
        "        delta_w = self.alpha * y * (X-y*self.weights) * self.delta_t\n",
        "        # Alternative equation of Oja's rule: delta_w = alpha * (C*w-w^T*C*w*w)\n",
        "        # cov = np.dot(X, X.transpose())\n",
        "        # delta_w = self.alpha * (np.dot(cov, self.weights)\n",
        "        #                         - np.dot(self.weights.transpose(),\n",
        "        #                                  np.dot(cov, self.weights))\n",
        "        #                         * self.weights)\n",
        "        self.weights += delta_w\n",
        "        return self.weights\n",
        "\n",
        "    def run(self, X):\n",
        "        \"\"\"\n",
        "        Iterate over all samples and apply Oja's rule on each step\n",
        "        \"\"\"\n",
        "        for sample in X:\n",
        "            self.t += self.delta_t\n",
        "            if self.dynamic_learning:\n",
        "                self.update_learning_rate()\n",
        "            self.update_weights(sample.reshape((2, 1)))\n",
        "            self.weights_trace = np.hstack((self.weights_trace, self.weights))\n",
        "        return"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCjM9nzet1GQ"
      },
      "source": [
        "Let's instantiate the previous class and run it with the simulated-spring data create before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvZKq6w6t6l4"
      },
      "source": [
        "# Apply Oja's rule on the samples and compare with PCA decomposition\n",
        "oja_network = OjaNetwork(dynamic_learning=False)\n",
        "oja_network.run(np.vstack((measured_displacement, measured_force)).transpose())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuxwcWqTuKY6"
      },
      "source": [
        "Let's now plot how the weights of the network evolve over the simulation, and compare them with the desired goal, which is the PC obtained with the standard method in the previous section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwg4ulsGuVBW"
      },
      "source": [
        "v_1 = pca.components_[0]\n",
        "fig, axs = plt.subplots(2, figsize=(10, 10))\n",
        "axs[0].set_title(\"Oja's rule vs. PCA\", fontsize=18)\n",
        "axs[0].plot(oja_network.weights_trace[0])\n",
        "# We use the abs function, because the standard method can provide a vector v or\n",
        "# v' pointing in the opposite direction. The sign is irrelevant in PCA, only the\n",
        "# direction matters\n",
        "axs[0].axhline(np.abs(v_1[0]), color=\"green\")\n",
        "\n",
        "axs[1].plot(oja_network.weights_trace[1])\n",
        "axs[1].axhline(np.abs(v_1[1]), color=\"green\")\n",
        "axs[1].set_xlabel(\"Iteration Nº\", fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTEbxHXQ1JI7"
      },
      "source": [
        "# Open questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rWnbR9iwPqS"
      },
      "source": [
        "* What happens if you modify the learning rate (self.alpha) dynamically? You can easily do this by setting ```dynamic_learning``` to True (remember to run the last three blocks of code after)\n",
        "* What is the impact of the learning rate? Try using higher and smaller values of self.alpha and see how it affects the final outcome. Which changes in the initial weights, initial learning rate, and learning rate function can facilitate a faster convergence to the desired values? At what cost, or which prior information would we need for implementing this changes?\n",
        "* Imagine that now you would like to obtain more than one principal component. How would you avoid all output neurons converging towards the main PC?"
      ]
    }
  ]
}